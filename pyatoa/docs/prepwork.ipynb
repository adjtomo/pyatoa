{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Inversion prep\n",
      "\n",
      "Here we provide a general template for setting up a tomographic inversion using ObsPy + Pyatoa. This will involve agathering an event catalog with moment tensors, collecting observation waveforms and response files, organizing data into the optimal directory structure, and generating ASDFDataSets that can be used in a SeisFlows or standalone inversion.\n",
      "\n",
      "\n",
      "## Alaska Event Catalog\n",
      "\n",
      "Alaska is a good region for an example problem, let's work there. First we'll use ObsPy to gather our initial catalog of events from the past decade in a box bounding Anchorage and Fairbanks."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from obspy import UTCDateTime, Catalog\n",
      "from obspy.clients.fdsn import Client"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = Client(\"USGS\")\n",
      "cat = c.get_events(starttime=UTCDateTime(\"2010-01-01T00:00:00\"), \n",
      "                   endtime=UTCDateTime(\"2020-01-01T00:00:00\"), \n",
      "                   maxdepth=60.0,\n",
      "                   minmagnitude=5.0,\n",
      "                   maxmagnitude=6.0, \n",
      "                   minlatitude=59.75, \n",
      "                   maxlatitude=65.50, \n",
      "                   minlongitude=-154.5, \n",
      "                   maxlongitude=-143.789\n",
      "                  )\n",
      "cat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets have a look at the Event IDs of our events. If we knew these apriori, we could have gathered our catalog based on event ids"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyatoa.utils.form import format_event_name\n",
      "\n",
      "event_ids = []\n",
      "for event in cat:\n",
      "    event_ids.append(format_event_name(event))\n",
      "event_ids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Getting moment tensors\n",
      "\n",
      "Great, we have an event catalog now, but to do waveform simulations we need moment tensors. \n",
      "\n",
      "Unfortunately it's not straight forward to grab moment tensor information directly from USGS as they do not directly provide XML files. It would be possible to manually generate moment tensor objects from each [individual event pages](https://earthquake.usgs.gov/earthquakes/eventpage/ak019lrs7iu/moment-tensor), but that seems tedious for a tutorial. \n",
      "\n",
      "Instead we'll use [GCMT](https://www.globalcmt.org/CMTsearch.html). Pyatoa has a function to read .ndk files hosted online with GCMT, finding events based on origintime and magnitude."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyatoa.core.gatherer import get_gcmt_moment_tensor\n",
      "\n",
      "events = []\n",
      "for event in cat:\n",
      "    origintime = event.preferred_origin().time\n",
      "    magnitude = event.preferred_magnitude().mag\n",
      "    try:\n",
      "        events.append(get_gcmt_moment_tensor(origintime, magnitude))\n",
      "    except FileNotFoundError:\n",
      "        print(f\"No GCMT event found for: {format_event_name(event)}\")\n",
      "        continue\n",
      "    \n",
      "gcmt_catalog = Catalog(events)\n",
      "print(f\"\\n{len(gcmt_catalog)}/{len(cat)} events with GCMT solutions found\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Great, 11 out of 15 isn't bad, we'll go ahead with and use the GCMT catalog that we just collected. However if we wanted to retain the (probably more accurate) origin information from the USGS catalog, we would need to move the moment tensor objects from the GCMT catalog over to the USGS catalog, an exercise left for the reader...\n",
      "\n",
      "## Gathering Observation Data\n",
      "\n",
      "Now we need seismic waveform data for all the events in our catalog. We can use the multithreaded data gathering functioality of Pyatoa's Gatherer class. First we need to determine the available broadband stations in the area, using ObsPy. \n",
      "\n",
      "Some pieces of relevant information that help motivate our search:\n",
      "*  The Alaska Earthquake Center (AEC) operates stations under the network code \"AK\".\n",
      "*  The SEED standard seismometer instrument code is \"H\"\n",
      "*  The SEED standard for broadband instruments is \"B\" or \"H\"\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = Client(\"IRIS\")\n",
      "inv = c.get_stations(network=\"AK\", \n",
      "                     station=\"*\", \n",
      "                     location=\"*\",\n",
      "                     channel=\"BH?\",\n",
      "                     starttime=UTCDateTime(\"2010-01-01T00:00:00\"), \n",
      "                     endtime=UTCDateTime(\"2020-01-01T00:00:00\"), \n",
      "                     minlatitude=59.75,                    \n",
      "                     maxlatitude=65.50, \n",
      "                     minlongitude=-154.5, \n",
      "                     maxlongitude=-143.789,\n",
      "                     level=\"channel\"\n",
      "                    )\n",
      "inv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We'll need to create a list of station ids for data gathering\n",
      "station_codes = []\n",
      "for net in inv:\n",
      "    for sta in net:\n",
      "        station_codes.append(f\"{net.code}.{sta.code}.*.BH?\")\n",
      "        \n",
      "# Let's just take a look at the first 10 as an example\n",
      "station_codes[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we look at the inventory we see that there are 76 available stations in our domain, quite a lot! Lets see how many have waveform data for the events in question. We will do this by creating an ASDFDataSet for a single event, and trying to fill it with all available data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyasdf import ASDFDataSet\n",
      "from pyatoa import Gatherer, Config"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Here we are just using the first event in our catalog\n",
      "event = gcmt_catalog[0]\n",
      "event_id = format_event_name(event)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The gatherer needs to know where to look (Client) and when to look (origintime)\n",
      "cfg = Config(client=\"IRIS\")\n",
      "origintime = event.preferred_origin().time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now we initate the Gatherer and use its multithreading capabilities to gather waveform and metadata\n",
      "# Here the 'return_count' argument means we only want to save stations that return data including \n",
      "# metadata (1) + 3 waveforms (3) = 4 \n",
      "with ASDFDataSet(f\"../tests/test_data/docs_data/{event_id}.h5\") as ds:\n",
      "    ds.add_quakeml(event)\n",
      "    gthr = Gatherer(config=cfg, ds=ds, origintime=origintime)\n",
      "    gthr.gather_obs_multithread(codes=station_codes, return_count=4, print_exception=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with ASDFDataSet(f\"../tests/test_data/docs_data/{event_id}.h5\") as ds:\n",
      "    print(ds.waveforms.list())\n",
      "    print(f\"\\n{len(ds.waveforms.list())} stations collected\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Great! Looks like we've got data for 41 stations just for this one event. Some stations did not return any data, as expected, but many of them returned a StationXML plus three component waveforms (as explained by data_count == 4).\n",
      "\n",
      "___ \n",
      "### Next Steps\n",
      "\n",
      "Now you can repeat the above data gathering steps for the remainder of the events in your catalog. Each event should get it's own ASDFDataSet to keep data organized nicely. Take a look at the Storage tutorial to get an idea of how to navigate and manipulate the ASDFDataSets. Also have a look at the Pyaflowa tutorial in order to figure out how to process the data you've just collected, either in a standalone manner using Pyatao + SPECFEM3D, or with an automated workflow tool like SeisFlows."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}