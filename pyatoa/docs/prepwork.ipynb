{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep work for Adjoint Tomography\n",
    "\n",
    "Here we provide a general template for setting up a tomographic inversion using ObsPy + Pyatoa. This will involve agathering an event catalog with moment tensors, collecting observation waveforms and response files, organizing data into the optimal directory structure, and generating ASDFDataSets that can be used in a SeisFlows or standalone inversion.\n",
    "\n",
    "\n",
    "## Event Catalog (Region: Alaska)\n",
    "\n",
    "Alaska's cool, let's work there. First we'll use ObsPy to gather our initial catalog of events from the past decade in a box bounding Anchorage and Fairbanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import UTCDateTime, Catalog\n",
    "from obspy.clients.fdsn import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15 Event(s) in Catalog:\n",
       "2019-01-13T16:45:55.437000Z | +61.299, -150.065 | 5.0 ml | manual\n",
       "2019-01-06T03:45:34.525000Z | +65.407, -153.280 | 5.1 ml | manual\n",
       "...\n",
       "2011-06-16T19:06:05.214000Z | +60.765, -151.076 | 5.1 mw | manual\n",
       "2011-01-23T02:50:04.629000Z | +63.542, -150.865 | 5.2 mw | manual\n",
       "To see all events call 'print(CatalogObject.__str__(print_all=True))'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Client(\"USGS\")\n",
    "cat = c.get_events(starttime=UTCDateTime(\"2010-01-01T00:00:00\"), \n",
    "                   endtime=UTCDateTime(\"2020-01-01T00:00:00\"), \n",
    "                   maxdepth=60.0,\n",
    "                   minmagnitude=5.0,\n",
    "                   maxmagnitude=6.0, \n",
    "                   minlatitude=59.75, \n",
    "                   maxlatitude=65.50, \n",
    "                   minlongitude=-154.5, \n",
    "                   maxlongitude=-143.789\n",
    "                  )\n",
    "cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the Event IDs of our events. If we knew these apriori, we could have gathered our catalog based on event ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ak019lrs7iu',\n",
       " 'ak0199za3yf',\n",
       " 'ak0191pccr7',\n",
       " 'ak018fe5jk85',\n",
       " 'ak20421672',\n",
       " 'ak018fcpk9xi',\n",
       " 'us1000hyge',\n",
       " 'ak018fcntv5m',\n",
       " 'ak018dsf3btv',\n",
       " 'ak017f7s3c06',\n",
       " 'ak014dlss56k',\n",
       " 'ak014b5xf1in',\n",
       " 'ak013ae2ycca',\n",
       " 'ak0117oi3hnt',\n",
       " 'ak011122ukq6']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyatoa.utils.form import format_event_name\n",
    "\n",
    "event_ids = []\n",
    "for event in cat:\n",
    "    event_ids.append(format_event_name(event))\n",
    "event_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting moment tensors\n",
    "\n",
    "Great, we have an event catalog now, but to do waveform simulations we need moment tensors. \n",
    "\n",
    "Unfortunately it's not straight forward to grab moment tensor information directly from USGS as they do not directly provide XML files. It would be possible to manually generate moment tensor objects from each [individual event pages](https://earthquake.usgs.gov/earthquakes/eventpage/ak019lrs7iu/moment-tensor), but that seems tedious for a tutorial. \n",
    "\n",
    "Instead we'll use [GCMT](https://www.globalcmt.org/CMTsearch.html). Pyatoa has a function to read .ndk files hosted online with GCMT, finding events based on origintime and magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GCMT event found for: ak018fcpk9xi\n",
      "No GCMT event found for: us1000hyge\n",
      "No GCMT event found for: ak018fcntv5m\n",
      "No GCMT event found for: ak013ae2ycca\n",
      "\n",
      "11/15 events with GCMT solutions found\n"
     ]
    }
   ],
   "source": [
    "from pyatoa.core.gatherer import get_gcmt_moment_tensor\n",
    "\n",
    "events = []\n",
    "for event in cat:\n",
    "    origintime = event.preferred_origin().time\n",
    "    magnitude = event.preferred_magnitude().mag\n",
    "    try:\n",
    "        events.append(get_gcmt_moment_tensor(origintime, magnitude))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No GCMT event found for: {format_event_name(event)}\")\n",
    "        continue\n",
    "    \n",
    "gcmt_catalog = Catalog(events)\n",
    "print(f\"\\n{len(gcmt_catalog)}/{len(cat)} events with GCMT solutions found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, 11 out of 15 isn't bad, we'll go ahead with and use the GCMT catalog that we just collected. However if we wanted to retain the (probably more accurate) origin information from the USGS catalog, we would need to move the moment tensor objects from the GCMT catalog over to the USGS catalog, an exercise left for the reader...\n",
    "\n",
    "## Gathering Observation Data\n",
    "\n",
    "Now we need seismic waveform data for all the events in our catalog. We can use the multithreaded data gathering functioality of Pyatoa's Gatherer class. First we need to determine the available broadband stations in the area, using ObsPy. \n",
    "\n",
    "Some pieces of relevant information that help motivate our search:\n",
    "*  The Alaska Earthquake Center (AEC) operates stations under the network code \"AK\".\n",
    "*  The SEED standard seismometer instrument code is \"H\"\n",
    "*  The SEED standard for broadband instruments is \"B\" or \"H\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inventory created at 2020-10-15T02:02:46.000000Z\n",
       "\tCreated by: IRIS WEB SERVICE: fdsnws-station | version: 1.1.46\n",
       "\t\t    http://service.iris.edu/fdsnws/station/1/query?starttime=2010-01-01...\n",
       "\tSending institution: IRIS-DMC (IRIS-DMC)\n",
       "\tContains:\n",
       "\t\tNetworks (1):\n",
       "\t\t\tAK\n",
       "\t\tStations (76):\n",
       "\t\t\tAK.BMR (Bremner River, AK, USA)\n",
       "\t\t\tAK.BPAW (Bear Paw Mountain, AK, USA)\n",
       "\t\t\tAK.BRLK (Bradley Lake, AK, USA)\n",
       "\t\t\tAK.BWN (Browne, AK, USA)\n",
       "\t\t\tAK.CAPN (Captain Cook Nikiski, AK, USA)\n",
       "\t\t\tAK.CAST (Castle Rocks, AK, USA)\n",
       "\t\t\tAK.CCB (Clear Creek Butte, AK, USA)\n",
       "\t\t\tAK.CHUM (Lake Minchumina, AK, USA)\n",
       "\t\t\tAK.CUT (Chulitna, AK, USA)\n",
       "\t\t\tAK.DDM (Donnely Dome, AK, USA)\n",
       "\t\t\tAK.DHY (Denali Highway, AK, USA)\n",
       "\t\t\tAK.DIV (Divide Microwave, AK, USA)\n",
       "\t\t\tAK.DOT (Dot Lake, AK, USA)\n",
       "\t\t\tAK.EYAK (Cordova Ski Area, AK, USA)\n",
       "\t\t\tAK.FIB (Fire Island, AK, USA)\n",
       "\t\t\tAK.FID (Fidalgo, AK, USA)\n",
       "\t\t\tAK.FIRE (Fire Island, AK, USA)\n",
       "\t\t\tAK.GHO (Gloryhole, AK, USA)\n",
       "\t\t\tAK.GLB (Gilahina Butte, AK, USA)\n",
       "\t\t\tAK.GLI (Glacier Island, AK, USA)\n",
       "\t\t\tAK.GLM (Gilmore Dome, AK, USA)\n",
       "\t\t\tAK.GOAT (Goat Mountain, AK, USA)\n",
       "\t\t\tAK.HDA (Harding Lake, AK, USA) (2x)\n",
       "\t\t\tAK.HIN (Hinchinbrook, AK, USA)\n",
       "\t\t\tAK.HMT (Hamilton, AK, USA)\n",
       "\t\t\tAK.I21K (Tanana, AK, USA)\n",
       "\t\t\tAK.I23K (Minto, Yukon-Koyukuk, AK, USA)\n",
       "\t\t\tAK.J20K (Nowitna River, AK, USA)\n",
       "\t\t\tAK.J25K (Salcha River, AK, USA)\n",
       "\t\t\tAK.K20K (Telida, AK, USA)\n",
       "\t\t\tAK.K24K (Donnelly Dome, AK, USA)\n",
       "\t\t\tAK.KAI (Kayak Island, AK, USA)\n",
       "\t\t\tAK.KLU (Klutina Pass, AK, USA)\n",
       "\t\t\tAK.KNK (Knik Glacier, AK, USA)\n",
       "\t\t\tAK.KTH (Kantishna Hills, AK, USA)\n",
       "\t\t\tAK.L20K (Farewell, AK, USA)\n",
       "\t\t\tAK.L22K (Petersville, AK, USA)\n",
       "\t\t\tAK.M19K (Big River Lodge, Big River, AK, USA)\n",
       "\t\t\tAK.M20K (Styx River, AK, USA)\n",
       "\t\t\tAK.MCK (McKinley Park, AK, USA)\n",
       "\t\t\tAK.MDM (Murphy Dome, AK, USA)\n",
       "\t\t\tAK.MLY (Manley Hot Springs, AK, USA)\n",
       "\t\t\tAK.N19K (Bonanza Creek NPS repeater, AK, USA)\n",
       "\t\t\tAK.NEA (Nenana, AK, USA)\n",
       "\t\t\tAK.NEA2 (Nenana, AK, USA)\n",
       "\t\t\tAK.NICH (Nichawak Mountain, AK, USA)\n",
       "\t\t\tAK.NKA (Nikiski, AK, USA)\n",
       "\t\t\tAK.O19K (Port Alsworth, AK, USA)\n",
       "\t\t\tAK.O20K (Slope Mountain, AK, USA)\n",
       "\t\t\tAK.P23K (Montague Island, AK, USA)\n",
       "\t\t\tAK.PAX (Paxson, AK, USA)\n",
       "\t\t\tAK.PPLA (Purkeypile, AK, USA)\n",
       "\t\t\tAK.PWL (Port Wells, AK, USA)\n",
       "\t\t\tAK.RAG (Ragged Mountain, AK, USA)\n",
       "\t\t\tAK.RC01 (Rabbit Creek, AK, USA)\n",
       "\t\t\tAK.RIDG (Independent Ridge, AK, USA)\n",
       "\t\t\tAK.RND (Reindeer, AK, USA)\n",
       "\t\t\tAK.SAW (Sawmill, AK, USA)\n",
       "\t\t\tAK.SCM (Sheep Mountain, AK, USA)\n",
       "\t\t\tAK.SCRK (Sand Creek, AK, USA)\n",
       "\t\t\tAK.SGA (Sherman Glacier, AK, USA)\n",
       "\t\t\tAK.SKN (Skwentna, AK, USA)\n",
       "\t\t\tAK.SLK (Skilak Lake, AK, USA)\n",
       "\t\t\tAK.SSN (Susitna, AK, USA)\n",
       "\t\t\tAK.SWD (Seward, AK, USA)\n",
       "\t\t\tAK.TRF (Thorofare Mountian, AK, USA) (2x)\n",
       "\t\t\tAK.WAT1 (Susitna Watana 1, AK, USA)\n",
       "\t\t\tAK.WAT2 (Susitna Watana 2, AK, USA)\n",
       "\t\t\tAK.WAT3 (Susitna Watana 3, AK, USA)\n",
       "\t\t\tAK.WAT4 (Susitna Watana 4, AK, USA)\n",
       "\t\t\tAK.WAT5 (Susitna Watana 5, AK, USA)\n",
       "\t\t\tAK.WAT6 (Susitna Watana 6, AK, USA)\n",
       "\t\t\tAK.WAT7 (Susitna Watana 7, AK, USA)\n",
       "\t\t\tAK.WRH (Wood River Hill, AK, USA)\n",
       "\t\tChannels (528):\n",
       "\t\t\tAK.BMR..BHZ (3x), AK.BMR..BHN (3x), AK.BMR..BHE (3x), \n",
       "\t\t\tAK.BPAW..BHZ (3x), AK.BPAW..BHN (3x), AK.BPAW..BHE (3x), \n",
       "\t\t\tAK.BRLK..BHZ (2x), AK.BRLK..BHN (2x), AK.BRLK..BHE (2x), \n",
       "\t\t\tAK.BWN..BHZ (3x), AK.BWN..BHN (3x), AK.BWN..BHE (3x), \n",
       "\t\t\tAK.CAPN..BHZ (2x), AK.CAPN..BHN (2x), AK.CAPN..BHE (2x), \n",
       "\t\t\tAK.CAST..BHZ (4x), AK.CAST..BHN (4x), AK.CAST..BHE (4x), \n",
       "\t\t\tAK.CCB..BHZ (3x), AK.CCB..BHN (3x), AK.CCB..BHE (3x), \n",
       "\t\t\tAK.CHUM..BHZ (2x), AK.CHUM..BHN (2x), AK.CHUM..BHE (2x), \n",
       "\t\t\tAK.CUT..BHZ (2x), AK.CUT..BHN (2x), AK.CUT..BHE (2x), \n",
       "\t\t\tAK.DDM..BHZ (2x), AK.DDM..BHN (2x), AK.DDM..BHE (2x), \n",
       "\t\t\tAK.DHY..BHZ (4x), AK.DHY..BHN (4x), AK.DHY..BHE (4x), \n",
       "\t\t\tAK.DIV..BHZ (4x), AK.DIV..BHN (4x), AK.DIV..BHE (4x), \n",
       "\t\t\tAK.DOT..BHZ (5x), AK.DOT..BHN (5x), AK.DOT..BHE (5x), \n",
       "\t\t\tAK.EYAK..BHZ (4x), AK.EYAK..BHN (4x), AK.EYAK..BHE (4x), \n",
       "\t\t\tAK.FIB..BHZ (2x), AK.FIB..BHN (2x), AK.FIB..BHE (2x), \n",
       "\t\t\tAK.FID..BHZ (4x), AK.FID..BHN (4x), AK.FID..BHE (4x), AK.FIRE..BHZ\n",
       "\t\t\tAK.FIRE..BHN, AK.FIRE..BHE, AK.GHO..BHZ, AK.GHO..BHN, AK.GHO..BHE\n",
       "\t\t\tAK.GLB..BHZ, AK.GLB..BHN, AK.GLB..BHE, AK.GLI..BHZ (3x), \n",
       "\t\t\tAK.GLI..BHN (3x), AK.GLI..BHE (3x), AK.GLM..BHZ, AK.GLM..BHN, \n",
       "\t\t\tAK.GLM..BHE, AK.GOAT..BHZ, AK.GOAT..BHN, AK.GOAT..BHE, \n",
       "\t\t\tAK.HDA..BHZ (2x), AK.HDA..BHN (2x), AK.HDA..BHE (2x), \n",
       "\t\t\tAK.HIN..BHZ (3x), AK.HIN..BHN (3x), AK.HIN..BHE (3x), \n",
       "\t\t\tAK.HMT..BHZ (2x), AK.HMT..BHN (2x), AK.HMT..BHE (2x), AK.I21K..BHZ\n",
       "\t\t\tAK.I21K..BHN, AK.I21K..BHE, AK.I23K..BHZ, AK.I23K..BHN, \n",
       "\t\t\tAK.I23K..BHE, AK.J20K..BHZ, AK.J20K..BHN, AK.J20K..BHE, \n",
       "\t\t\tAK.J25K..BHZ, AK.J25K..BHN, AK.J25K..BHE, AK.K20K..BHZ, \n",
       "\t\t\tAK.K20K..BHN, AK.K20K..BHE, AK.K24K..BHZ, AK.K24K..BHN, \n",
       "\t\t\tAK.K24K..BHE, AK.KAI..BHZ (2x), AK.KAI..BHN (2x), AK.KAI..BHE (2x)\n",
       "\t\t\tAK.KLU..BHZ (2x), AK.KLU..BHN (2x), AK.KLU..BHE (2x), \n",
       "\t\t\tAK.KNK..BHZ (2x), AK.KNK..BHN (2x), AK.KNK..BHE (2x), \n",
       "\t\t\tAK.KTH..BHZ (2x), AK.KTH..BHN (2x), AK.KTH..BHE (2x), AK.L20K..BHZ\n",
       "\t\t\tAK.L20K..BHN, AK.L20K..BHE, AK.L22K..BHZ, AK.L22K..BHN, \n",
       "\t\t\tAK.L22K..BHE, AK.M19K..BHZ, AK.M19K..BHN, AK.M19K..BHE, \n",
       "\t\t\tAK.M20K..BHZ, AK.M20K..BHN, AK.M20K..BHE, AK.MCK..BHZ (3x), \n",
       "\t\t\tAK.MCK..BHN (3x), AK.MCK..BHE (3x), AK.MDM..BHZ (8x), \n",
       "\t\t\tAK.MDM..BHN (8x), AK.MDM..BHE (8x), AK.MLY..BHZ (4x), \n",
       "\t\t\tAK.MLY..BHN (4x), AK.MLY..BHE (4x), AK.N19K..BHZ, AK.N19K..BHN, \n",
       "\t\t\tAK.N19K..BHE, AK.NEA..BHZ, AK.NEA..BHN, AK.NEA..BHE, AK.NEA2..BHZ, \n",
       "\t\t\tAK.NEA2..BHN, AK.NEA2..BHE, AK.NICH..BHZ, AK.NICH..BHN, \n",
       "\t\t\tAK.NICH..BHE, AK.NKA..BHZ, AK.NKA..BHN, AK.NKA..BHE, AK.O19K..BHZ, \n",
       "\t\t\tAK.O19K..BHN, AK.O19K..BHE, AK.O20K..BHZ, AK.O20K..BHN, \n",
       "\t\t\tAK.O20K..BHE, AK.P23K..BHZ, AK.P23K..BHN, AK.P23K..BHE, \n",
       "\t\t\tAK.PAX..BHZ (4x), AK.PAX..BHN (4x), AK.PAX..BHE (4x), \n",
       "\t\t\tAK.PPLA..BHZ (2x), AK.PPLA..BHN (2x), AK.PPLA..BHE (2x), \n",
       "\t\t\tAK.PWL..BHZ (3x), AK.PWL..BHN (3x), AK.PWL..BHE (3x), \n",
       "\t\t\tAK.RAG..BHZ (3x), AK.RAG..BHN (3x), AK.RAG..BHE (3x), \n",
       "\t\t\tAK.RC01..BHZ (2x), AK.RC01..BHN (2x), AK.RC01..BHE (2x), \n",
       "\t\t\tAK.RIDG..BHZ (3x), AK.RIDG..BHN (3x), AK.RIDG..BHE (3x), \n",
       "\t\t\tAK.RND..BHZ (3x), AK.RND..BHN (3x), AK.RND..BHE (3x), \n",
       "\t\t\tAK.SAW..BHZ (4x), AK.SAW..BHN (4x), AK.SAW..BHE (4x), \n",
       "\t\t\tAK.SCM..BHZ (2x), AK.SCM..BHN (2x), AK.SCM..BHE (2x), \n",
       "\t\t\tAK.SCRK..BHZ (2x), AK.SCRK..BHN (2x), AK.SCRK..BHE (2x), \n",
       "\t\t\tAK.SGA..BHZ (4x), AK.SGA..BHN (4x), AK.SGA..BHE (4x), \n",
       "\t\t\tAK.SKN..BHZ (4x), AK.SKN..BHN (4x), AK.SKN..BHE (4x), AK.SLK..BHZ, \n",
       "\t\t\tAK.SLK..BHN, AK.SLK..BHE, AK.SSN..BHZ (5x), AK.SSN..BHN (5x), \n",
       "\t\t\tAK.SSN..BHE (5x), AK.SWD..BHZ (3x), AK.SWD..BHN (3x), \n",
       "\t\t\tAK.SWD..BHE (3x), AK.TRF..BHZ (4x), AK.TRF..BHN (4x), \n",
       "\t\t\tAK.TRF..BHE (4x), AK.WAT1..BHZ (3x), AK.WAT1..BHN (3x), \n",
       "\t\t\tAK.WAT1..BHE (3x), AK.WAT2..BHZ (3x), AK.WAT2..BHN (3x), \n",
       "\t\t\tAK.WAT2..BHE (3x), AK.WAT3..BHZ (4x), AK.WAT3..BHN (4x), \n",
       "\t\t\tAK.WAT3..BHE (4x), AK.WAT4..BHZ (4x), AK.WAT4..BHN (4x), \n",
       "\t\t\tAK.WAT4..BHE (4x), AK.WAT5..BHZ (2x), AK.WAT5..BHN (2x), \n",
       "\t\t\tAK.WAT5..BHE (2x), AK.WAT6..BHZ (2x), AK.WAT6..BHN (2x), \n",
       "\t\t\tAK.WAT6..BHE (2x), AK.WAT7..BHZ (2x), AK.WAT7..BHN (2x), \n",
       "\t\t\tAK.WAT7..BHE (2x), AK.WRH..BHZ (2x), AK.WRH..BHN (2x), \n",
       "\t\t\tAK.WRH..BHE (2x)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Client(\"IRIS\")\n",
    "inv = c.get_stations(network=\"AK\", \n",
    "                     station=\"*\", \n",
    "                     location=\"*\",\n",
    "                     channel=\"BH?\",\n",
    "                     starttime=UTCDateTime(\"2010-01-01T00:00:00\"), \n",
    "                     endtime=UTCDateTime(\"2020-01-01T00:00:00\"), \n",
    "                     minlatitude=59.75,                    \n",
    "                     maxlatitude=65.50, \n",
    "                     minlongitude=-154.5, \n",
    "                     maxlongitude=-143.789,\n",
    "                     level=\"channel\"\n",
    "                    )\n",
    "inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AK.BMR.*.BH?',\n",
       " 'AK.BPAW.*.BH?',\n",
       " 'AK.BRLK.*.BH?',\n",
       " 'AK.BWN.*.BH?',\n",
       " 'AK.CAPN.*.BH?',\n",
       " 'AK.CAST.*.BH?',\n",
       " 'AK.CCB.*.BH?',\n",
       " 'AK.CHUM.*.BH?',\n",
       " 'AK.CUT.*.BH?',\n",
       " 'AK.DDM.*.BH?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll need to create a list of station ids for data gathering\n",
    "station_codes = []\n",
    "for net in inv:\n",
    "    for sta in net:\n",
    "        station_codes.append(f\"{net.code}.{sta.code}.*.BH?\")\n",
    "        \n",
    "# Let's just take a look at the first 10 as an example\n",
    "station_codes[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the inventory we see that there are 76 available stations in our domain, quite a lot! Lets see how many have waveform data for the events in question. We will do this by creating an ASDFDataSet for a single event, and trying to fill it with all available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyasdf import ASDFDataSet\n",
    "from pyatoa import Gatherer, Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are just using the first event in our catalog\n",
    "event = gcmt_catalog[0]\n",
    "event_id = format_event_name(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gatherer needs to know where to look (Client) and when to look (origintime)\n",
    "cfg = Config(client=\"IRIS\")\n",
    "origintime = event.preferred_origin().time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AK.DDM.*.BH? data count: 0\n",
      "AK.FIB.*.BH? data count: 0\n",
      "AK.FID.*.BH? data count: 1\n",
      "AK.GLI.*.BH? data count: 1\n",
      "AK.DHY.*.BH? data count: 1\n",
      "AK.GLM.*.BH? data count: 0\n",
      "AK.CAST.*.BH? data count: 4\n",
      "AK.GLB.*.BH? data count: 4\n",
      "AK.EYAK.*.BH? data count: 4\n",
      "AK.BMR.*.BH? data count: 4\n",
      "AK.DOT.*.BH? data count: 4\n",
      "AK.DIV.*.BH? data count: 4\n",
      "AK.CCB.*.BH? data count: 4\n",
      "AK.CHUM.*.BH? data count: 4\n",
      "AK.BPAW.*.BH? data count: 4\n",
      "AK.CUT.*.BH? data count: 4\n",
      "AK.GHO.*.BH? data count: 4\n",
      "AK.BWN.*.BH? data count: 4\n",
      "AK.I21K.*.BH? data count: 0\n",
      "AK.I23K.*.BH? data count: 0\n",
      "AK.BRLK.*.BH? data count: 4\n",
      "AK.CAPN.*.BH? data count: 4\n",
      "AK.J20K.*.BH? data count: 0\n",
      "AK.K24K.*.BH? data count: 0\n",
      "AK.K20K.*.BH? data count: 0\n",
      "AK.J25K.*.BH? data count: 0\n",
      "AK.HDA.*.BH? data count: 4\n",
      "AK.HDA.*.BH? exception: Unable to create link (name already exists)\n",
      "\n",
      "AK.GOAT.*.BH? data count: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Chow/miniconda3/envs/tomo/lib/python3.7/site-packages/pyasdf/asdf_data_set.py:1546: ASDFWarning: Data 'AK.HDA/AK.HDA..BHE__2019-01-13T16:45:35__2019-01-13T16:54:15__observed' already exists in file. Will not be added!\n",
      "  warnings.warn(msg, ASDFWarning)\n",
      "/Users/Chow/miniconda3/envs/tomo/lib/python3.7/site-packages/pyasdf/asdf_data_set.py:1546: ASDFWarning: Data 'AK.HDA/AK.HDA..BHN__2019-01-13T16:45:35__2019-01-13T16:54:15__observed' already exists in file. Will not be added!\n",
      "  warnings.warn(msg, ASDFWarning)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Chow/Documents/academic/vuw/packages/pyatoa/pyatoa/core/gatherer.py\", line 812, in gather_obs_multithread\n",
      "    status = future.result()\n",
      "  File \"/Users/Chow/miniconda3/envs/tomo/lib/python3.7/concurrent/futures/_base.py\", line 425, in result\n",
      "    return self.__get_result()\n",
      "  File \"/Users/Chow/miniconda3/envs/tomo/lib/python3.7/concurrent/futures/_base.py\", line 384, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/Users/Chow/miniconda3/envs/tomo/lib/python3.7/concurrent/futures/thread.py\", line 57, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/Chow/Documents/academic/vuw/packages/pyatoa/pyatoa/core/gatherer.py\", line 227, in _obs_get_multithread\n",
      "    self.ds.add_waveforms(waveform=st, tag=self.config.observed_tag)\n",
      "  File \"/Users/Chow/miniconda3/envs/tomo/lib/python3.7/site-packages/pyasdf/asdf_data_set.py\", line 1328, in add_waveforms\n",
      "    self._add_trace_write_collective_information(info)\n",
      "  File \"/Users/Chow/miniconda3/envs/tomo/lib/python3.7/site-packages/pyasdf/asdf_data_set.py\", line 1482, in _add_trace_write_collective_information\n",
      "    ds = group.create_dataset(**info[\"dataset_creation_params\"])\n",
      "  File \"/Users/Chow/miniconda3/envs/tomo/lib/python3.7/site-packages/h5py/_hl/group.py\", line 139, in create_dataset\n",
      "    self[name] = dset\n",
      "  File \"/Users/Chow/miniconda3/envs/tomo/lib/python3.7/site-packages/h5py/_hl/group.py\", line 373, in __setitem__\n",
      "    h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5o.pyx\", line 202, in h5py.h5o.link\n",
      "OSError: Unable to create link (name already exists)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AK.L20K.*.BH? data count: 0\n",
      "AK.HIN.*.BH? data count: 4\n",
      "AK.L22K.*.BH? data count: 0\n",
      "AK.FIRE.*.BH? data count: 4\n",
      "AK.M19K.*.BH? data count: 0\n",
      "AK.M20K.*.BH? data count: 0\n",
      "AK.MDM.*.BH? data count: 0\n",
      "AK.NEA.*.BH? data count: 0\n",
      "AK.N19K.*.BH? data count: 0\n",
      "AK.NKA.*.BH? data count: 0\n",
      "AK.O19K.*.BH? data count: 0\n",
      "AK.MLY.*.BH? data count: 1\n",
      "AK.O20K.*.BH? data count: 0\n",
      "AK.P23K.*.BH? data count: 0\n",
      "AK.HMT.*.BH? data count: 4\n",
      "AK.SGA.*.BH? data count: 0\n",
      "AK.KAI.*.BH? data count: 4\n",
      "AK.KLU.*.BH? data count: 4\n",
      "AK.KTH.*.BH? data count: 4\n",
      "AK.SSN.*.BH? data count: 1\n",
      "AK.KNK.*.BH? data count: 4\n",
      "AK.MCK.*.BH? data count: 4\n",
      "AK.NEA2.*.BH? data count: 4\n",
      "AK.PAX.*.BH? data count: 4\n",
      "AK.WAT1.*.BH? data count: 0\n",
      "AK.WAT2.*.BH? data count: 0\n",
      "AK.PPLA.*.BH? data count: 4\n",
      "AK.NICH.*.BH? data count: 4\n",
      "AK.WAT3.*.BH? data count: 0\n",
      "AK.SCRK.*.BH? data count: 4\n",
      "AK.SAW.*.BH? data count: 4\n",
      "AK.WAT4.*.BH? data count: 0\n",
      "AK.PWL.*.BH? data count: 4\n",
      "AK.RND.*.BH? data count: 4\n",
      "AK.WAT5.*.BH? data count: 0\n",
      "AK.RIDG.*.BH? data count: 4\n",
      "AK.WAT6.*.BH? data count: 1\n",
      "AK.SCM.*.BH? data count: 4\n",
      "AK.RC01.*.BH? data count: 4\n",
      "AK.SKN.*.BH? data count: 4\n",
      "AK.WAT7.*.BH? data count: 1\n",
      "AK.RAG.*.BH? data count: 4\n",
      "AK.SLK.*.BH? data count: 4\n",
      "AK.TRF.*.BH? data count: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Chow/miniconda3/envs/tomo/lib/python3.7/site-packages/pyasdf/asdf_data_set.py:1546: ASDFWarning: Data 'AK.TRF/AK.TRF..BHE__2019-01-13T16:45:35__2019-01-13T16:54:15__observed' already exists in file. Will not be added!\n",
      "  warnings.warn(msg, ASDFWarning)\n",
      "/Users/Chow/miniconda3/envs/tomo/lib/python3.7/site-packages/pyasdf/asdf_data_set.py:1546: ASDFWarning: Data 'AK.TRF/AK.TRF..BHN__2019-01-13T16:45:35__2019-01-13T16:54:15__observed' already exists in file. Will not be added!\n",
      "  warnings.warn(msg, ASDFWarning)\n",
      "/Users/Chow/miniconda3/envs/tomo/lib/python3.7/site-packages/pyasdf/asdf_data_set.py:1546: ASDFWarning: Data 'AK.TRF/AK.TRF..BHZ__2019-01-13T16:45:35__2019-01-13T16:54:15__observed' already exists in file. Will not be added!\n",
      "  warnings.warn(msg, ASDFWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AK.TRF.*.BH? data count: 4\n",
      "AK.SWD.*.BH? data count: 4\n",
      "AK.WRH.*.BH? data count: 4\n"
     ]
    }
   ],
   "source": [
    "# Now we initate the Gatherer and use its multithreading capabilities to gather waveform and metadata\n",
    "# Here the 'return_count' argument means we only want to save stations that return data including \n",
    "# metadata (1) + 3 waveforms (3) = 4 \n",
    "with ASDFDataSet(f\"../tests/test_data/scratch/{event_id}.h5\") as ds:\n",
    "    ds.add_quakeml(event)\n",
    "    gthr = Gatherer(config=cfg, ds=ds, origintime=origintime)\n",
    "    gthr.gather_obs_multithread(codes=station_codes, return_count=4, print_exception=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AK.BMR', 'AK.BPAW', 'AK.BRLK', 'AK.BWN', 'AK.CAPN', 'AK.CAST', 'AK.CCB', 'AK.CHUM', 'AK.CUT', 'AK.DIV', 'AK.DOT', 'AK.EYAK', 'AK.FIRE', 'AK.GHO', 'AK.GLB', 'AK.GOAT', 'AK.HDA', 'AK.HIN', 'AK.HMT', 'AK.KAI', 'AK.KLU', 'AK.KNK', 'AK.KTH', 'AK.MCK', 'AK.NEA2', 'AK.NICH', 'AK.PAX', 'AK.PPLA', 'AK.PWL', 'AK.RAG', 'AK.RC01', 'AK.RIDG', 'AK.RND', 'AK.SAW', 'AK.SCM', 'AK.SCRK', 'AK.SKN', 'AK.SLK', 'AK.SWD', 'AK.TRF', 'AK.WRH']\n",
      "\n",
      "41 stations collected\n"
     ]
    }
   ],
   "source": [
    "with ASDFDataSet(f\"../tests/test_data/scratch/{event_id}.h5\") as ds:\n",
    "    print(ds.waveforms.list())\n",
    "    print(f\"\\n{len(ds.waveforms.list())} stations collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Looks like we've got data for 41 stations just for this one event. Some stations did not return any data, as expected, but many of them returned a StationXML plus three component waveforms (as explained by data_count == 4).\n",
    "\n",
    "___ \n",
    "### Next Steps\n",
    "\n",
    "Now you can repeat the above data gathering steps for the remainder of the events in your catalog. Each event should get it's own ASDFDataSet to keep data organized nicely. Take a look at the Storage tutorial to get an idea of how to navigate and manipulate the ASDFDataSets. Also have a look at the Pyaflowa tutorial in order to figure out how to process the data you've just collected, either in a standalone manner using Pyatao + SPECFEM3D, or with an automated workflow tool like SeisFlows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
